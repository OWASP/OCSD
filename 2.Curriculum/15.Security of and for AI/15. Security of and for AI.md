# Module 15. Security of and for AI

## Learning Objectives

After completing this module, developers will be able to:

- Identify and mitigate OWASP LLM Top 10 (2025) vulnerabilities
- Implement defenses against prompt injection and system prompt leakage
- Secure vector databases and RAG implementations against embedding attacks
- Apply controls for excessive agency, unbounded consumption, and misinformation risks
- Design guardrails and content filtering for AI-generated outputs

---

## 15.1 OWASP LLM Top 10

### Overview of AI/LLM Risks (2025)

- LLM01:2025 Prompt Injection
- LLM02:2025 Sensitive Information Disclosure
- LLM03:2025 Supply Chain
- LLM04:2025 Data and Model Poisoning
- LLM05:2025 Improper Output Handling
- LLM06:2025 Excessive Agency
- LLM07:2025 System Prompt Leakage
- LLM08:2025 Vector and Embedding Weaknesses
- LLM09:2025 Misinformation
- LLM10:2025 Unbounded Consumption

---

## 15.2 Prompt Injection Attacks

### Direct Prompt Injection

- User input manipulation of prompts
- Instruction override attacks
- Jailbreaking attempts
- Role-playing exploits

### Indirect Prompt Injection

- Malicious content in retrieved data
- Poisoned external sources
- Hidden instructions in documents
- Cross-plugin attacks

### Prevention Techniques

- Input sanitization and validation
- Prompt structure hardening
- Delimiter strategies
- Output validation
- Least privilege for LLM actions

---

## 15.3 Input Data Validation

### Validating User Inputs

- Length and format restrictions
- Content filtering
- Rate limiting prompts
- Context window management

### Validating Retrieved Data

- Source verification
- Content scanning before inclusion
- Sandboxing external content
- Trust boundaries for data sources

---

## 15.4 Output Data Control

### Insecure Output Handling

- XSS through AI-generated content
- Code injection via outputs
- Markdown/HTML rendering risks
- Command execution from outputs

### Output Validation

- Sanitization before rendering
- Format validation
- Content safety filtering
- Human review for sensitive actions

---

## 15.5 Training Data Poisoning

### Attack Vectors

- Malicious training data injection
- Data source compromise
- Label manipulation
- Backdoor insertion

### Prevention

- Training data validation
- Source verification
- Anomaly detection in training
- Model behavior monitoring

---

## 15.6 Model Theft and Extraction

### Extraction Techniques

- Query-based model extraction
- API abuse for model cloning
- Side-channel attacks
- Insider threats

### Protection Measures

- Rate limiting and monitoring
- Query pattern analysis
- Watermarking techniques
- Access controls for model APIs

---

## 15.7 System Prompt Leakage

### Understanding System Prompts

- Role of system prompts in LLM behavior
- Sensitive information in system prompts
- Business logic encoded in prompts
- API keys and credentials in prompts

### Leakage Attack Vectors

- Direct extraction attempts
- Prompt reflection techniques
- Behavioral inference attacks
- Error message disclosure

### Prevention Strategies

- Avoid storing secrets in system prompts
- Prompt obfuscation techniques
- Output filtering for prompt content
- Separation of instructions from sensitive data
- Regular prompt auditing

---

## 15.8 Sensitive Information Disclosure

### Disclosure Risks

- Training data leakage
- PII in model outputs
- Confidential information exposure
- System prompt leakage

### Mitigation Strategies

- Data minimization in training
- Output filtering for sensitive data
- Differential privacy techniques
- Prompt confidentiality measures

---

## 15.9 Supply Chain Risks for AI Models

### Supply Chain Threats

- Compromised pre-trained models
- Malicious model weights
- Poisoned fine-tuning data
- Vulnerable dependencies

### Secure Model Management

- Model provenance verification
- Integrity checking (checksums, signatures)
- Trusted model sources
- Vulnerability scanning of AI libraries

---

## 15.10 Vector and Embedding Weaknesses

### Understanding Vector Databases and RAG

- Role of embeddings in LLM applications
- Vector databases for retrieval-augmented generation (RAG)
- Semantic search and similarity matching
- Knowledge base integration

### Vulnerability Categories

- Unauthorized access to vector stores
- Data poisoning in embeddings
- Embedding inversion attacks
- Cross-tenant data leakage
- Adversarial embedding manipulation

### Security Controls

- Access control for vector databases
- Encryption of embeddings at rest and in transit
- Input validation before embedding generation
- Monitoring for anomalous queries
- Tenant isolation in multi-tenant systems
- Regular auditing of embedded content

---

## 15.11 Misinformation

### Understanding AI Misinformation Risks

- Hallucinations and confabulations
- Authoritative presentation of false information
- Outdated training data issues
- Overreliance on AI outputs

### Impact Areas

- Decision-making based on false information
- Reputation and legal risks
- User trust erosion
- Compliance violations

### Mitigation Strategies

- Fact-checking integration
- Source attribution requirements
- Confidence scoring for outputs
- Human verification workflows
- Clear AI disclosure to users
- Grounding with verified data sources
- Regular model evaluation for accuracy

---

## 15.12 Unbounded Consumption

### Resource Exhaustion Attacks

- Denial of wallet attacks (cost exploitation)
- Computational resource exhaustion
- Context window manipulation
- Recursive or infinite loop triggers

### Attack Vectors

- Excessive token generation requests
- Complex query flooding
- Large context window exploitation
- Concurrent request abuse

### Prevention Controls

- Token and cost limits per user/session
- Request rate limiting
- Timeout enforcement
- Query complexity analysis
- Budget alerts and caps
- Resource quotas per tenant
- Monitoring for abnormal usage patterns

---

## 15.13 AI-Generated Code Security

### Risks of AI-Generated Code

- Insecure code suggestions
- Outdated or deprecated patterns
- Vulnerable dependencies
- Injection vulnerabilities

### Secure Integration

- Code review requirements
- Automated security scanning
- Testing AI-generated code
- Developer security training

---

## 15.14 Guardrails and Content Filtering

### Input Guardrails

- Topic restrictions
- Harmful content detection
- Prompt classification
- Policy enforcement

### Output Guardrails

- Safety filters
- Factuality checking
- Bias detection
- Toxicity prevention

### Implementation Approaches

- Rule-based filters
- ML-based content moderation
- Human-in-the-loop for sensitive outputs
- Layered defense strategies

---

## 15.15 Model Access Controls and Rate Limiting

### Access Control

- Authentication for model APIs
- Authorization levels
- Scope restrictions
- Audit logging

### Rate Limiting

- Request throttling
- Token/cost limits
- Abuse detection
- Capacity management

---

## 15.16 Excessive Agency

### Understanding Excessive Agency

- LLMs with autonomous action capabilities
- Plugin and tool integration risks
- Unintended action execution
- Scope creep in AI permissions

### Risk Scenarios

- Unauthorized data modifications
- Unintended external communications
- Financial transactions without approval
- System configuration changes

### Control Measures

- Principle of least privilege for AI actions
- Human-in-the-loop for sensitive operations
- Action scope limitations
- Approval workflows for high-risk actions
- Comprehensive action logging
- Rollback capabilities
- Clear boundaries for autonomous behavior

---

## 15.17 Identities, Access Control and Privilege Escalation

### AI System Identity Management

- Service account security
- API key management
- OAuth for AI services
- Cross-service authentication

### Privilege Management

- Least privilege for AI agents
- Capability restrictions
- Action approval workflows
- Sandboxing AI operations

---

## 15.18 Privacy Requirements for AI

### Data Privacy

- Training data privacy
- User prompt confidentiality
- Data retention policies
- Right to deletion

### Regulatory Compliance

- GDPR considerations for AI
- AI-specific regulations
- Consent for AI processing
- Transparency requirements
